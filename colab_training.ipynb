{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77cb3589",
   "metadata": {},
   "source": [
    "# Child Speech Recognition - Google Colab Training\n",
    "\n",
    "This notebook trains a Whisper-small model on children's speech data using Google Colab's free GPU.\n",
    "\n",
    "**Before starting:**\n",
    "1. Enable GPU: `Runtime` → `Change runtime type` → `T4 GPU`\n",
    "2. Upload data to Google Drive (see instructions below)\n",
    "3. Run cells sequentially\n",
    "\n",
    "**Estimated time:**\n",
    "- Setup: 5 minutes\n",
    "- Quick test: 5-10 minutes  \n",
    "- Full training: 3-6 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92778f36",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/ekshubina/childs_speech_recog_chall.git\n",
    "%cd childs_speech_recog_chall\n",
    "\n",
    "# Install dependencies (takes ~2 minutes)\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Set PYTHONPATH so 'src' module is importable in subprocesses\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '/content/childs_speech_recog_chall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40749e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU and PyTorch installation\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"Enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
    "\n",
    "# Verify src module is importable\n",
    "try:\n",
    "    from src.utils.config import load_config\n",
    "    print(\"✓ src module loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ERROR: Cannot import src module: {e}\")\n",
    "    print(\"Re-run the setup cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f9e8b",
   "metadata": {},
   "source": [
    "### Update Code from Git\n",
    "\n",
    "Run this cell if you've pushed changes to GitHub and need to update the code in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa16a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/childs_speech_recog_chall\n",
    "!git pull origin main\n",
    "\n",
    "# Reinstall requirements if dependencies changed\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"✓ Code updated from GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf01a48",
   "metadata": {},
   "source": [
    "## 2. Load Data via Bore Tunnel\n",
    "\n",
    "Transfer data from your Mac directly to Colab — no Google Drive needed.\n",
    "\n",
    "**On your Mac, open two terminals:**\n",
    "\n",
    "**Terminal 1** — serve your data directory:\n",
    "```bash\n",
    "cd /path/to/childs_speech_recog_chall/data\n",
    "python3 -m http.server 8080\n",
    "```\n",
    "\n",
    "**Terminal 2** — expose it with bore:\n",
    "```bash\n",
    "bore local 8080 --to bore.pub\n",
    "# Prints: listening at bore.pub:XXXXX  ← copy that port number\n",
    "```\n",
    "\n",
    "Then set `BORE_PORT` in the cell below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists('data'):\n",
    "    !rm -r data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ← Set your bore port number here\n",
    "BORE_PORT = \"XXXXX\"\n",
    "BORE_URL = f\"http://bore.pub:{BORE_PORT}\"\n",
    "\n",
    "!mkdir -p data\n",
    "\n",
    "# Download zip files from your Mac\n",
    "!wget -q --show-progress \"{BORE_URL}/audio_0.zip\" -O data/audio_0.zip\n",
    "!wget -q --show-progress \"{BORE_URL}/audio_1.zip\" -O data/audio_1.zip\n",
    "!wget -q --show-progress \"{BORE_URL}/audio_2.zip\" -O data/audio_2.zip\n",
    "!wget -q --show-progress \"{BORE_URL}/train_word_transcripts.jsonl\" -O data/train_word_transcripts.jsonl\n",
    "\n",
    "print(\"✓ Downloads complete\")\n",
    "# Extract zip files\n",
    "# Note: assumes each zip extracts to audio_0/, audio_1/, audio_2/ inside data/\n",
    "# Run: !unzip -l data/audio_0.zip | head -5   to check structure if unsure\n",
    "!unzip -q data/audio_0.zip -d data/\n",
    "!unzip -q data/audio_1.zip -d data/\n",
    "!unzip -q data/audio_2.zip -d data/\n",
    "\n",
    "# Free up disk space\n",
    "# !rm data/audio_*.zip\n",
    "\n",
    "# Verify\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path('data/train_word_transcripts.jsonl')\n",
    "if manifest_path.exists():\n",
    "    with open(manifest_path) as f:\n",
    "        sample_count = sum(1 for _ in f)\n",
    "    print(f\"✓ Found training manifest with {sample_count:,} samples\")\n",
    "else:\n",
    "    print(\"❌ ERROR: Training manifest not found!\")\n",
    "\n",
    "for audio_dir in ['audio_0', 'audio_1', 'audio_2']:\n",
    "    audio_path = Path(f'data/{audio_dir}')\n",
    "    if audio_path.exists():\n",
    "        file_count = len(list(audio_path.glob('**/*.flac')))\n",
    "        print(f\"✓ Found data/{audio_dir}/ with {file_count:,} audio files\")\n",
    "    else:\n",
    "        print(f\"❌ ERROR: data/{audio_dir}/ not found! Check zip structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6ef8c",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive\n",
    "\n",
    "Checkpoints are saved locally during training (fast I/O), then copied to Drive afterwards.  \n",
    "**Run this before the quick test and full training.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c904d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and symlink checkpoint directory\n",
    "# The trainer writes to a local path, which is a symlink → Drive.\n",
    "# Checkpoints land on Drive in real time — no copy step needed.\n",
    "import os, yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Always anchor to repo root — prevents CWD-relative path bugs after runtime restarts\n",
    "REPO_ROOT = Path(\"/content/childs_speech_recog_chall\")\n",
    "os.chdir(REPO_ROOT)\n",
    "print(f\"✓ CWD: {os.getcwd()}\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_CHECKPOINT_DIR = \"/content/drive/MyDrive/childs_speech_recog_chall/checkpoints\"\n",
    "LOCAL_CHECKPOINT_DIR = str(REPO_ROOT / \"checkpoints\")  # symlink → Drive\n",
    "\n",
    "# Create the real directory on Drive\n",
    "os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink local path → Drive (so trainer writes straight to Drive)\n",
    "if not Path(LOCAL_CHECKPOINT_DIR).exists():\n",
    "    os.symlink(DRIVE_CHECKPOINT_DIR, LOCAL_CHECKPOINT_DIR)\n",
    "    print(f\"✓ Symlink created: {LOCAL_CHECKPOINT_DIR} → {DRIVE_CHECKPOINT_DIR}\")\n",
    "elif Path(LOCAL_CHECKPOINT_DIR).is_symlink():\n",
    "    print(f\"✓ Symlink already exists: {LOCAL_CHECKPOINT_DIR} → {os.readlink(LOCAL_CHECKPOINT_DIR)}\")\n",
    "else:\n",
    "    # A real directory exists — replace it with a symlink\n",
    "    import shutil\n",
    "    shutil.rmtree(LOCAL_CHECKPOINT_DIR)\n",
    "    os.symlink(DRIVE_CHECKPOINT_DIR, LOCAL_CHECKPOINT_DIR)\n",
    "    print(f\"✓ Replaced local dir with symlink → {DRIVE_CHECKPOINT_DIR}\")\n",
    "\n",
    "# --- Build patched config and write it to an absolute path ---\n",
    "RUN_NAME       = \"baseline_whisper_small\"\n",
    "# Use absolute path so the correct file is always written regardless of CWD\n",
    "config_patched = str(REPO_ROOT / \"configs\" / \"baseline_whisper_small_local.yaml\")\n",
    "\n",
    "with open(REPO_ROOT / \"configs\" / \"baseline_whisper_small.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "local_output = f\"{LOCAL_CHECKPOINT_DIR}/{RUN_NAME}\"\n",
    "cfg[\"training\"][\"output_dir\"]  = local_output\n",
    "cfg[\"training\"][\"logging_dir\"] = f\"{local_output}/runs\"\n",
    "\n",
    "# Memory optimizations for T4 GPU (15 GB):\n",
    "# - batch_size=4 + grad_accum=10 → effective batch = 40 (≈ original 12*3=36)\n",
    "# - freeze_encoder skips storing encoder gradients (~60% of params) → ~4 GB saved\n",
    "# - eval_batch_size=8 is safe (no gradients needed during eval)\n",
    "≈\n",
    "cfg[\"model\"][\"freeze_encoder\"]                 = True\n",
    "\n",
    "with open(config_patched, \"w\") as f:\n",
    "    yaml.dump(cfg, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "# --- Verify the file on disk matches what we wrote ---\n",
    "with open(config_patched) as f:\n",
    "    verify = yaml.safe_load(f)\n",
    "\n",
    "assert verify[\"training\"][\"batch_size\"] == 4, \\\n",
    "    f\"❌ batch_size write failed! Got {verify['training']['batch_size']}\"\n",
    "assert verify[\"training\"][\"gradient_accumulation_steps\"] == 10, \\\n",
    "    f\"❌ grad_accum write failed! Got {verify['training']['gradient_accumulation_steps']}\"\n",
    "assert verify[\"model\"][\"freeze_encoder\"] == True, \\\n",
    "    f\"❌ freeze_encoder write failed! Got {verify['model']['freeze_encoder']}\"\n",
    "\n",
    "print(f\"✓ Config written and verified: {config_patched}\")\n",
    "print(f\"  batch_size            = {verify['training']['batch_size']}\")\n",
    "print(f\"  gradient_accumulation = {verify['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"  effective_batch       = {verify['training']['batch_size'] * verify['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"  eval_batch_size       = {verify['training']['eval_batch_size']}\")\n",
    "print(f\"  freeze_encoder        = {verify['model']['freeze_encoder']}\")\n",
    "print(f\"✓ Checkpoints → {local_output}  (symlinked to Drive, saved in real time)\")\n",
    "print(f\"✓ On Drive: MyDrive/childs_speech_recog_chall/checkpoints/{RUN_NAME}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab87e4e",
   "metadata": {},
   "source": [
    "## 4. Quick Test (Recommended First Step)\n",
    "\n",
    "Run a quick test on 100 samples to verify everything works before starting full training.  \n",
    "After it finishes, run the **\"Copy to Drive\"** cell to confirm files appear on Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test — trains on 100 samples to verify the pipeline (~5-10 min)\n",
    "# Checkpoints are written directly to Drive via symlink — nothing to copy.\n",
    "!PYTORCH_ALLOC_CONF=expandable_segments:True python scripts/train.py --config {config_patched} --debug\n",
    "\n",
    "from pathlib import Path\n",
    "drive_output = f\"{DRIVE_CHECKPOINT_DIR}/{RUN_NAME}\"\n",
    "if Path(drive_output).exists():\n",
    "    print(f\"\\n✓ Checkpoints on Drive: MyDrive/childs_speech_recog_chall/checkpoints/{RUN_NAME}/\")\n",
    "    print(f\"   {[p.name for p in sorted(Path(drive_output).iterdir())]}\")\n",
    "else:\n",
    "    print(\"⚠️ No checkpoints found — check logs above for errors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff828c",
   "metadata": {},
   "source": [
    "## 5. Full Training\n",
    "\n",
    "⚠️ **This will take 3-6 hours** depending on GPU speed. Make sure:\n",
    "- You have GPU enabled (Runtime → Change runtime type → T4 GPU)\n",
    "- **Re-run the Drive mount cell (cell 11)** to apply memory optimizations to the config\n",
    "- Checkpoints are written to Drive in real time via symlink — safe even if session disconnects\n",
    "\n",
    "To **resume** from a previous checkpoint:\n",
    "```python\n",
    "!PYTORCH_ALLOC_CONF=expandable_segments:True python scripts/train.py \\\n",
    "    --config {config_patched} \\\n",
    "    --resume {LOCAL_CHECKPOINT_DIR}/{RUN_NAME}/checkpoint-XXXX\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training — checkpoints are written directly to Drive via symlink.\n",
    "# To resume from a previous run, set --resume to a checkpoint path on Drive:\n",
    "#   --resume {LOCAL_CHECKPOINT_DIR}/{RUN_NAME}/checkpoint-XXXX\n",
    "!PYTORCH_ALLOC_CONF=expandable_segments:True python scripts/train.py --config {config_patched}\n",
    "\n",
    "from pathlib import Path\n",
    "drive_output = f\"{DRIVE_CHECKPOINT_DIR}/{RUN_NAME}\"\n",
    "if Path(drive_output).exists():\n",
    "    print(f\"\\n✓ Checkpoints on Drive: MyDrive/childs_speech_recog_chall/checkpoints/{RUN_NAME}/\")\n",
    "    print(f\"   {[p.name for p in sorted(Path(drive_output).iterdir())]}\")\n",
    "else:\n",
    "    print(\"⚠️ No checkpoints found — check logs above for errors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816736c",
   "metadata": {},
   "source": [
    "## 6. Monitor Training with TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard — reads from local logs while training, Drive logs after copy\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {LOCAL_CHECKPOINT_DIR}/baseline_whisper_small/runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6e1f0",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set — uses model from Google Drive\n",
    "MODEL_PATH = f\"{DRIVE_CHECKPOINT_DIR}/baseline_whisper_small/final_model\"\n",
    "\n",
    "!python scripts/evaluate.py \\\n",
    "    --model-path {MODEL_PATH} \\\n",
    "    --val-manifest data/val_manifest.jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130ebea",
   "metadata": {},
   "source": [
    "## 8. Generate Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "!python scripts/predict.py \\\n",
    "    --model-path {MODEL_PATH} \\\n",
    "    --input-jsonl data/test_manifest.jsonl \\\n",
    "    --output-jsonl predictions.jsonl \\\n",
    "    --batch-size 16\n",
    "\n",
    "# Copy predictions to Drive\n",
    "!cp predictions.jsonl /content/drive/MyDrive/predictions.jsonl\n",
    "\n",
    "print(\"✓ Predictions saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd1c54",
   "metadata": {},
   "source": [
    "## 9. Download Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download predictions file to your local machine\n",
    "from google.colab import files\n",
    "files.download('predictions.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d5c02",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### No `childs_speech_recog_chall` folder on Google Drive\n",
    "If the folder is missing, re-run the **Drive mount cell** (cell 11) to remount Drive and recreate the symlink and variables. Checkpoints are saved directly to Drive in real time via the symlink — no copy step needed.\n",
    "\n",
    "### Session Timeout During Training\n",
    "If your Colab session disconnects, checkpoints already saved are safe on Drive. Resume from the last checkpoint:\n",
    "```python\n",
    "RESUME_CKPT = \"/content/drive/MyDrive/childs_speech_recog_chall/checkpoints/baseline_whisper_small/checkpoint-XXXX\"\n",
    "!PYTORCH_ALLOC_CONF=expandable_segments:True python scripts/train.py \\\n",
    "    --config configs/baseline_whisper_small_local.yaml \\\n",
    "    --resume {RESUME_CKPT}\n",
    "```\n",
    "\n",
    "### Out of Memory (OOM)\n",
    "The Drive mount cell (cell 11) patches the config to use memory-safe settings for the T4 GPU:\n",
    "- `batch_size=4`, `gradient_accumulation_steps=10` → effective batch = 40\n",
    "- `freeze_encoder=True` → skips encoder gradients, saves ~4 GB\n",
    "- `PYTORCH_ALLOC_CONF=expandable_segments:True` → reduces fragmentation\n",
    "\n",
    "**You must re-run cell 11 to regenerate `configs/baseline_whisper_small_local.yaml` before training.**  \n",
    "If you still get OOM after re-running cell 11, reduce batch size further by editing cell 11:\n",
    "```python\n",
    "cfg[\"training\"][\"batch_size\"]                  = 2\n",
    "cfg[\"training\"][\"gradient_accumulation_steps\"] = 20\n",
    "```\n",
    "Then re-run cell 11 and restart training.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
